{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26321bf5",
   "metadata": {},
   "source": [
    "# 2 Data wrangling<a id='2_Data_wrangling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc75536",
   "metadata": {},
   "source": [
    "## 2.1 Contents<a id='2.1_Contents'></a>\n",
    "* [2 Data wrangling](#2_Data_wrangling)\n",
    "  * [2.1 Contents](#2.1_Contents)\n",
    "  * [2.2 Introduction](#2.2_Introduction)\n",
    "    * [2.2.1 Data Science Problem](#2.2.1_Data_Science_Problem)\n",
    "  * [2.3 Imports](#2.3_Imports)\n",
    "  * [2.4 Load Data](#2.4_Load_Data)\n",
    "  * [2.5 Missing Values](#2.5_Missing_Values)\n",
    "  * [2.6 Duplicate Values](#2.6_Duplicate_Values)\n",
    "    * [2.6.1 Duplicate Records](#2.6.1_Duplicate_Records)\n",
    "    * [2.6.2 Duplicate Reviews](#2.6.2_Duplicate_Reviews)\n",
    "  * [2.7 Creating a Helpfulness Ratio](#2.7.2_Helpfulness_Ratio)\n",
    "  * [2.8 Converting Time to DateTime](#2.8_Converting_Time)\n",
    "  * [2.9 Text Preprocessing](#2.9_Text_Preprocessing)\n",
    "  * [2.10 Tokenization and Stopword Removal](#2.10_Tokenization)\n",
    "  * [2.11 Lemmatization](#2.11_Lemmatization)\n",
    "  * [2.12 Stemming](#2.12_Stemming)    \n",
    "  * [2.13 Saving the Cleaned Data](#2.13_Saving)\n",
    "  * [2.14 Summary](#2.14_Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19293835",
   "metadata": {},
   "source": [
    "## 2.2 Introduction<a id='2.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2bd30",
   "metadata": {},
   "source": [
    "Data wrangling is the process of cleaning, transforming, and organizing raw data into a usable format. Below are the steps for performing data wrangling on the \"Amazon Fine Food Reviews\" dataset with the columns Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d42938",
   "metadata": {},
   "source": [
    "### 2.2.1 Data Science Problem<a id='2.2.1_Data_Science_Problem'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1b7c3",
   "metadata": {},
   "source": [
    "The \"Amazon Fine Food Reviews\" dataset offers valuable insights into customer sentiment and preferences through product reviews. By applying natural language processing (NLP) and machine learning techniques, the challenge is to analyze review texts, ratings, and other attributes to identify patterns and trends in customer feedback. This will help uncover key themes related to product quality, delivery experience, and overall satisfaction. The goal is to provide actionable insights for improving product offerings, enhancing customer service, and informing marketing strategies, ultimately leading to better customer understanding and increased loyalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d646c28",
   "metadata": {},
   "source": [
    "## 2.3 Imports<a id='2.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dff0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sqlite3\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2422ef",
   "metadata": {},
   "source": [
    "## 2.4 Load Data<a id='2.4_Load_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc478875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165256</th>\n",
       "      <td>165257</td>\n",
       "      <td>B000EVG8J2</td>\n",
       "      <td>A1L01D2BD3RKVO</td>\n",
       "      <td>B. Miller \"pet person\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1268179200</td>\n",
       "      <td>Crunchy &amp; Good Gluten-Free Sandwich Cookies!</td>\n",
       "      <td>Having tried a couple of other brands of glute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231465</th>\n",
       "      <td>231466</td>\n",
       "      <td>B0000BXJIS</td>\n",
       "      <td>A3U62RE5XZDP0G</td>\n",
       "      <td>Marty</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1298937600</td>\n",
       "      <td>great kitty treats</td>\n",
       "      <td>My cat loves these treats. If ever I can't fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427827</th>\n",
       "      <td>427828</td>\n",
       "      <td>B008FHUFAU</td>\n",
       "      <td>AOXC0JQQZGGB6</td>\n",
       "      <td>Kenneth Shevlin</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1224028800</td>\n",
       "      <td>COFFEE TASTE</td>\n",
       "      <td>A little less than I expected.  It tends to ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433954</th>\n",
       "      <td>433955</td>\n",
       "      <td>B006BXV14E</td>\n",
       "      <td>A3PWPNZVMNX3PA</td>\n",
       "      <td>rareoopdvds</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1335312000</td>\n",
       "      <td>So the Mini-Wheats were too big?</td>\n",
       "      <td>First there was Frosted Mini-Wheats, in origin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70260</th>\n",
       "      <td>70261</td>\n",
       "      <td>B007I7Z3Z0</td>\n",
       "      <td>A1XNZ7PCE45KK7</td>\n",
       "      <td>Og8ys1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1334707200</td>\n",
       "      <td>Great Taste . . .</td>\n",
       "      <td>and I want to congratulate the graphic artist ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId             ProfileName  \\\n",
       "165256  165257  B000EVG8J2  A1L01D2BD3RKVO  B. Miller \"pet person\"   \n",
       "231465  231466  B0000BXJIS  A3U62RE5XZDP0G                   Marty   \n",
       "427827  427828  B008FHUFAU   AOXC0JQQZGGB6         Kenneth Shevlin   \n",
       "433954  433955  B006BXV14E  A3PWPNZVMNX3PA             rareoopdvds   \n",
       "70260    70261  B007I7Z3Z0  A1XNZ7PCE45KK7                  Og8ys1   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "165256                     0                       0      5  1268179200   \n",
       "231465                     0                       0      5  1298937600   \n",
       "427827                     0                       2      3  1224028800   \n",
       "433954                     0                       1      2  1335312000   \n",
       "70260                      0                       2      5  1334707200   \n",
       "\n",
       "                                             Summary  \\\n",
       "165256  Crunchy & Good Gluten-Free Sandwich Cookies!   \n",
       "231465                            great kitty treats   \n",
       "427827                                  COFFEE TASTE   \n",
       "433954              So the Mini-Wheats were too big?   \n",
       "70260                              Great Taste . . .   \n",
       "\n",
       "                                                     Text  \n",
       "165256  Having tried a couple of other brands of glute...  \n",
       "231465  My cat loves these treats. If ever I can't fin...  \n",
       "427827  A little less than I expected.  It tends to ha...  \n",
       "433954  First there was Frosted Mini-Wheats, in origin...  \n",
       "70260   and I want to congratulate the graphic artist ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "amazon_data = pd.read_csv(\"Reviews.csv\")\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "amazon_data = amazon_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Limit to the first 5000 records\n",
    "amazon_data = amazon_data.head(100000)\n",
    "\n",
    "# Display the first few records to verify\n",
    "amazon_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00223e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con = sqlite3.connect('database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191c3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews LIMIT 10000\"\"\", con)\n",
    "# amazon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177d7f8",
   "metadata": {},
   "source": [
    "## 2.5 Missing Values<a id='2.5_Missing_Values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c442c3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                        0\n",
      "ProductId                 0\n",
      "UserId                    0\n",
      "ProfileName               2\n",
      "HelpfulnessNumerator      0\n",
      "HelpfulnessDenominator    0\n",
      "Score                     0\n",
      "Time                      0\n",
      "Summary                   9\n",
      "Text                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = amazon_data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7530322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'ProfileName' with 'Unknown'\n",
    "amazon_data['ProfileName'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Drop rows where 'Text' is missing, as text data is crucial\n",
    "amazon_data.dropna(subset=['Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be2b54",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369938f",
   "metadata": {},
   "source": [
    "## 2.6 Duplicate Values<a id='2.6_Duplicate_Values'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf7820",
   "metadata": {},
   "source": [
    "### 2.6.1 Duplicate Records<a id='2.6.1_Duplicate_Records'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2622f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for duplicates, excluding 'Id'\n",
    "columns_to_check = [col for col in amazon_data.columns if col != 'Id']\n",
    "\n",
    "# Check for duplicate records based on all columns except 'Id'\n",
    "duplicate_records = amazon_data[amazon_data.duplicated(subset=columns_to_check, keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c9f958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170486</th>\n",
       "      <td>170487</td>\n",
       "      <td>B000MXEN9O</td>\n",
       "      <td>A3O0VDZUOJPZWX</td>\n",
       "      <td>Orlando Mom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1313020800</td>\n",
       "      <td>Best price anywhere!</td>\n",
       "      <td>My little one loves these and you cannot beat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81058</th>\n",
       "      <td>81059</td>\n",
       "      <td>B000MXHQW0</td>\n",
       "      <td>AO29VDV2AUM6W</td>\n",
       "      <td>Desiree Calora</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1319068800</td>\n",
       "      <td>Great baby food</td>\n",
       "      <td>This is great for your little one, no artifici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325098</th>\n",
       "      <td>325099</td>\n",
       "      <td>B0002DGRRA</td>\n",
       "      <td>AJD41FBJD9010</td>\n",
       "      <td>N. Ferguson \"Two, Daisy, Hannah, and Kitten\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1233360000</td>\n",
       "      <td>best dog treat-- great for training---  all do...</td>\n",
       "      <td>Freeze dried liver has a hypnotic effect on do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170484</th>\n",
       "      <td>170485</td>\n",
       "      <td>B000MXEN9O</td>\n",
       "      <td>A3O0VDZUOJPZWX</td>\n",
       "      <td>Orlando Mom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1313020800</td>\n",
       "      <td>Best price anywhere!</td>\n",
       "      <td>My little one loves these and you cannot beat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255267</th>\n",
       "      <td>255268</td>\n",
       "      <td>B0029NVJX8</td>\n",
       "      <td>A3LCQXQ4SFYBAU</td>\n",
       "      <td>Johna Jane</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1345420800</td>\n",
       "      <td>Cat's favorite</td>\n",
       "      <td>These treats are my picky cat's favorite.  I'v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  \\\n",
       "170486  170487  B000MXEN9O  A3O0VDZUOJPZWX   \n",
       "81058    81059  B000MXHQW0   AO29VDV2AUM6W   \n",
       "325098  325099  B0002DGRRA   AJD41FBJD9010   \n",
       "170484  170485  B000MXEN9O  A3O0VDZUOJPZWX   \n",
       "255267  255268  B0029NVJX8  A3LCQXQ4SFYBAU   \n",
       "\n",
       "                                         ProfileName  HelpfulnessNumerator  \\\n",
       "170486                                   Orlando Mom                     0   \n",
       "81058                                 Desiree Calora                     0   \n",
       "325098  N. Ferguson \"Two, Daisy, Hannah, and Kitten\"                     0   \n",
       "170484                                   Orlando Mom                     0   \n",
       "255267                                    Johna Jane                     0   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "170486                       0      5  1313020800   \n",
       "81058                        0      5  1319068800   \n",
       "325098                       0      5  1233360000   \n",
       "170484                       0      5  1313020800   \n",
       "255267                       0      5  1345420800   \n",
       "\n",
       "                                                  Summary  \\\n",
       "170486                               Best price anywhere!   \n",
       "81058                                     Great baby food   \n",
       "325098  best dog treat-- great for training---  all do...   \n",
       "170484                               Best price anywhere!   \n",
       "255267                                     Cat's favorite   \n",
       "\n",
       "                                                     Text  \n",
       "170486  My little one loves these and you cannot beat ...  \n",
       "81058   This is great for your little one, no artifici...  \n",
       "325098  Freeze dried liver has a hypnotic effect on do...  \n",
       "170484  My little one loves these and you cannot beat ...  \n",
       "255267  These treats are my picky cat's favorite.  I'v...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b7b88d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_records.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72addaa",
   "metadata": {},
   "source": [
    "There are a total of 506 duplicate records identified in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f0110",
   "metadata": {},
   "source": [
    "### 2.6.2 Duplicate Reviews<a id='2.6.2_Duplicate_Reviews'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc38a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store non-duplicate records\n",
    "non_duplicate_records = amazon_data.drop_duplicates(keep=False)\n",
    "\n",
    "# Check for duplicate records based on ProductId, UserId, and ProfileName\n",
    "duplicate_reviews = non_duplicate_records[non_duplicate_records.duplicated(subset=['ProductId', 'UserId', 'ProfileName', 'Time'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a41a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41443</th>\n",
       "      <td>41444</td>\n",
       "      <td>B0088YBUOU</td>\n",
       "      <td>A37REIKYSHU4ZF</td>\n",
       "      <td>Miles Hiniker</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1199750400</td>\n",
       "      <td>Yum</td>\n",
       "      <td>These potatoes are good.  But i don't know if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64007</th>\n",
       "      <td>64008</td>\n",
       "      <td>B000NHYNNU</td>\n",
       "      <td>A36RGOVNCCA5KI</td>\n",
       "      <td>M. Barrow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1194566400</td>\n",
       "      <td>Good stuff</td>\n",
       "      <td>I decided to try this tea because I am a big f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282916</th>\n",
       "      <td>282917</td>\n",
       "      <td>B002Z9EQPO</td>\n",
       "      <td>A2ISKAWUPGGOLZ</td>\n",
       "      <td>M. S. Handley</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1310774400</td>\n",
       "      <td>Kitty Junk Food</td>\n",
       "      <td>We have five cats - one an elderly cat of 15 y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188194</th>\n",
       "      <td>188195</td>\n",
       "      <td>B001NXM3I0</td>\n",
       "      <td>A2BT4NQWOUS5C0</td>\n",
       "      <td>sandybeaches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1283299200</td>\n",
       "      <td>Tasty and Healthy</td>\n",
       "      <td>I love that my son is learning to feed himself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8534</th>\n",
       "      <td>8535</td>\n",
       "      <td>B003VXFK44</td>\n",
       "      <td>A12BJ9GOL0T54E</td>\n",
       "      <td>Andy L. \"NC_Andy\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1308182400</td>\n",
       "      <td>Didn't say anywhere it was coconut favored!</td>\n",
       "      <td>It's coconut flavored, either you'll like it o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId        ProfileName  \\\n",
       "41443    41444  B0088YBUOU  A37REIKYSHU4ZF      Miles Hiniker   \n",
       "64007    64008  B000NHYNNU  A36RGOVNCCA5KI          M. Barrow   \n",
       "282916  282917  B002Z9EQPO  A2ISKAWUPGGOLZ      M. S. Handley   \n",
       "188194  188195  B001NXM3I0  A2BT4NQWOUS5C0       sandybeaches   \n",
       "8534      8535  B003VXFK44  A12BJ9GOL0T54E  Andy L. \"NC_Andy\"   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "41443                      2                       2      3  1199750400   \n",
       "64007                      0                       0      5  1194566400   \n",
       "282916                     0                       1      1  1310774400   \n",
       "188194                     0                       0      5  1283299200   \n",
       "8534                       0                       0      3  1308182400   \n",
       "\n",
       "                                            Summary  \\\n",
       "41443                                           Yum   \n",
       "64007                                    Good stuff   \n",
       "282916                              Kitty Junk Food   \n",
       "188194                            Tasty and Healthy   \n",
       "8534    Didn't say anywhere it was coconut favored!   \n",
       "\n",
       "                                                     Text  \n",
       "41443   These potatoes are good.  But i don't know if ...  \n",
       "64007   I decided to try this tea because I am a big f...  \n",
       "282916  We have five cats - one an elderly cat of 15 y...  \n",
       "188194  I love that my son is learning to feed himself...  \n",
       "8534    It's coconut flavored, either you'll like it o...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59edde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a20dd",
   "metadata": {},
   "source": [
    "There are 7,424 duplicate reviews in the dataset for the same product (identified by ProductId), submitted by the same user (identified by UserId) at the same time (identified by Time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cd060",
   "metadata": {},
   "source": [
    "## 2.7 Creating a Helpfulness Ratio<a id='2.7.2_Helpfulness_Ratio'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39840823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the helpfulness ratio\n",
    "amazon_data['HelpfulnessRatio'] = amazon_data['HelpfulnessNumerator'] / amazon_data['HelpfulnessDenominator']\n",
    "\n",
    "# Handle division by zero and missing values by filling with 0\n",
    "amazon_data['HelpfulnessRatio'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5733233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000    53360\n",
       "1.000000    32304\n",
       "0.500000     3894\n",
       "0.666667     1798\n",
       "0.750000     1127\n",
       "            ...  \n",
       "0.523810        1\n",
       "0.931596        1\n",
       "0.656250        1\n",
       "0.045455        1\n",
       "0.190476        1\n",
       "Name: HelpfulnessRatio, Length: 478, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data['HelpfulnessRatio'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28402c0",
   "metadata": {},
   "source": [
    "The output shows the distribution of the HelpfulnessRatio in the amazon_data DataFrame. The data reveals that a significant number of reviews are marked as either completely unhelpful (0.0) or entirely helpful (1.0), with counts of 303,826 and 183,309 respectively. Intermediate values, such as 0.5, 0.67, and 0.75, appear less frequently, indicating some reviews receive mixed feedback. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b2c37d",
   "metadata": {},
   "source": [
    "## 2.8 Converting Time to DateTime<a id='2.8_Converting_Time'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3fadd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Time column to datetime\n",
    "amazon_data['ReviewTime'] = pd.to_datetime(amazon_data['Time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9168f79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012-10-16    206\n",
       "2012-09-06    187\n",
       "2011-11-25    185\n",
       "2012-08-16    181\n",
       "2012-08-06    177\n",
       "             ... \n",
       "2006-07-28      1\n",
       "2003-07-02      1\n",
       "2006-01-19      1\n",
       "2004-05-10      1\n",
       "2005-08-09      1\n",
       "Name: ReviewTime, Length: 2699, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data['ReviewTime'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb39db",
   "metadata": {},
   "source": [
    "October 16, 2012, has the highest frequency of customer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1db38ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest review times 1999-12-06 00:00:00 and the latest review times 2012-10-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('The earliest review times', amazon_data['ReviewTime'].min(), 'and the latest review times', amazon_data['ReviewTime'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d2787",
   "metadata": {},
   "source": [
    "## 2.9 Text Preprocessing<a id='2.9_Text_Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f1c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the clean_text function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, float):\n",
    "        text = str(text)\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "\n",
    "# Ensure the Text and Summary columns are strings\n",
    "amazon_data['Text'] = amazon_data['Text'].astype(str)\n",
    "amazon_data['Summary'] = amazon_data['Summary'].astype(str)\n",
    "\n",
    "# Apply the clean_text function to the Text and Summary columns\n",
    "amazon_data['CleanedText'] = amazon_data['Text'].apply(clean_text)\n",
    "amazon_data['CleanedSummary'] = amazon_data['Summary'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfdfa5",
   "metadata": {},
   "source": [
    "The preprocessing of text data converts the 'Text' and 'Summary' columns in the amazon_data dataframe to lowercase, and removes digits, punctuation, extra spaces, and leading/trailing spaces to create cleaned versions of these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019c745",
   "metadata": {},
   "source": [
    "## 2.10 Tokenization and Stopword Removal<a id='2.10_Tokenization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e70c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\armeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\armeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure you have the stopwords and punkt data downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to tokenize and remove stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply the tokenize_text function to the CleanedText and CleanedSummary columns\n",
    "amazon_data['TokenizedText'] = amazon_data['CleanedText'].apply(tokenize_text)\n",
    "amazon_data['TokenizedSummary'] = amazon_data['CleanedSummary'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dedb81",
   "metadata": {},
   "source": [
    "The code tokenizes the cleaned text data from the 'CleanedText' and 'CleanedSummary' columns in the amazon_data dataframe and removes stopwords, creating tokenized versions of these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fead6",
   "metadata": {},
   "source": [
    "## 2.11 Lemmatization<a id='2.11_Lemmatization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056fe6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\armeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply the lemmatize_tokens function to the TokenizedText and TokenizedSummary columns\n",
    "amazon_data['LemmatizedText'] = amazon_data['TokenizedText'].apply(lemmatize_tokens)\n",
    "amazon_data['LemmatizedSummary'] = amazon_data['TokenizedSummary'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76d450",
   "metadata": {},
   "source": [
    "The code lemmatizes the tokenized text data from the 'TokenizedText' and 'TokenizedSummary' columns in the amazon_data dataframe, creating lemmatized versions of these columns to ensure words are reduced to their base or root form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59160de2",
   "metadata": {},
   "source": [
    "## 2.12 Stemming<a id='2.12_Lemmatization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "948e6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to stem text\n",
    "def stem_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Apply stemming to the 'Summary' and 'Text' columns\n",
    "amazon_data['StemmedSummary'] = amazon_data['Summary'].apply(stem_text)\n",
    "amazon_data['StemmedText'] = amazon_data['Text'].apply(stem_text)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Stemmed Amazon Data\", dataframe=amazon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b96a65",
   "metadata": {},
   "source": [
    "This code applies stemming to the tokens in the tokens column of the amazon_data DataFrame using the Porter Stemmer from the NLTK library, and stores the resulting stemmed tokens in a new column called stemmed_tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d85e6",
   "metadata": {},
   "source": [
    "## 2.13 Saving the Cleaned Data<a id='2.13_Saving'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b30b078f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>HelpfulnessRatio</th>\n",
       "      <th>ReviewTime</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>CleanedSummary</th>\n",
       "      <th>TokenizedText</th>\n",
       "      <th>TokenizedSummary</th>\n",
       "      <th>LemmatizedText</th>\n",
       "      <th>LemmatizedSummary</th>\n",
       "      <th>StemmedSummary</th>\n",
       "      <th>StemmedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165256</th>\n",
       "      <td>165257</td>\n",
       "      <td>B000EVG8J2</td>\n",
       "      <td>A1L01D2BD3RKVO</td>\n",
       "      <td>B. Miller \"pet person\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1268179200</td>\n",
       "      <td>Crunchy &amp; Good Gluten-Free Sandwich Cookies!</td>\n",
       "      <td>Having tried a couple of other brands of glute...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010-03-10</td>\n",
       "      <td>having tried a couple of other brands of glute...</td>\n",
       "      <td>crunchy  good glutenfree sandwich cookies</td>\n",
       "      <td>[tried, couple, brands, glutenfree, sandwich, ...</td>\n",
       "      <td>[crunchy, good, glutenfree, sandwich, cookies]</td>\n",
       "      <td>[tried, couple, brand, glutenfree, sandwich, c...</td>\n",
       "      <td>[crunchy, good, glutenfree, sandwich, cooky]</td>\n",
       "      <td>crunchi &amp; good gluten-fre sandwich cooki !</td>\n",
       "      <td>have tri a coupl of other brand of gluten-fre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231465</th>\n",
       "      <td>231466</td>\n",
       "      <td>B0000BXJIS</td>\n",
       "      <td>A3U62RE5XZDP0G</td>\n",
       "      <td>Marty</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1298937600</td>\n",
       "      <td>great kitty treats</td>\n",
       "      <td>My cat loves these treats. If ever I can't fin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>my cat loves these treats if ever i cant find ...</td>\n",
       "      <td>great kitty treats</td>\n",
       "      <td>[cat, loves, treats, ever, cant, find, house, ...</td>\n",
       "      <td>[great, kitty, treats]</td>\n",
       "      <td>[cat, love, treat, ever, cant, find, house, po...</td>\n",
       "      <td>[great, kitty, treat]</td>\n",
       "      <td>great kitti treat</td>\n",
       "      <td>my cat love these treat . if ever i ca n't fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427827</th>\n",
       "      <td>427828</td>\n",
       "      <td>B008FHUFAU</td>\n",
       "      <td>AOXC0JQQZGGB6</td>\n",
       "      <td>Kenneth Shevlin</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1224028800</td>\n",
       "      <td>COFFEE TASTE</td>\n",
       "      <td>A little less than I expected.  It tends to ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2008-10-15</td>\n",
       "      <td>a little less than i expected  it tends to hav...</td>\n",
       "      <td>coffee taste</td>\n",
       "      <td>[little, less, expected, tends, muddy, taste, ...</td>\n",
       "      <td>[coffee, taste]</td>\n",
       "      <td>[little, le, expected, tends, muddy, taste, ex...</td>\n",
       "      <td>[coffee, taste]</td>\n",
       "      <td>coffe tast</td>\n",
       "      <td>a littl less than i expect . it tend to have a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433954</th>\n",
       "      <td>433955</td>\n",
       "      <td>B006BXV14E</td>\n",
       "      <td>A3PWPNZVMNX3PA</td>\n",
       "      <td>rareoopdvds</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1335312000</td>\n",
       "      <td>So the Mini-Wheats were too big?</td>\n",
       "      <td>First there was Frosted Mini-Wheats, in origin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-04-25</td>\n",
       "      <td>first there was frosted miniwheats in original...</td>\n",
       "      <td>so the miniwheats were too big</td>\n",
       "      <td>[first, frosted, miniwheats, original, size, f...</td>\n",
       "      <td>[miniwheats, big]</td>\n",
       "      <td>[first, frosted, miniwheats, original, size, f...</td>\n",
       "      <td>[miniwheats, big]</td>\n",
       "      <td>so the mini-wheat were too big ?</td>\n",
       "      <td>first there wa frost mini-wheat , in origin si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70260</th>\n",
       "      <td>70261</td>\n",
       "      <td>B007I7Z3Z0</td>\n",
       "      <td>A1XNZ7PCE45KK7</td>\n",
       "      <td>Og8ys1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1334707200</td>\n",
       "      <td>Great Taste . . .</td>\n",
       "      <td>and I want to congratulate the graphic artist ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-04-18</td>\n",
       "      <td>and i want to congratulate the graphic artist ...</td>\n",
       "      <td>great taste</td>\n",
       "      <td>[want, congratulate, graphic, artist, putting,...</td>\n",
       "      <td>[great, taste]</td>\n",
       "      <td>[want, congratulate, graphic, artist, putting,...</td>\n",
       "      <td>[great, taste]</td>\n",
       "      <td>great tast . . .</td>\n",
       "      <td>and i want to congratul the graphic artist for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId             ProfileName  \\\n",
       "165256  165257  B000EVG8J2  A1L01D2BD3RKVO  B. Miller \"pet person\"   \n",
       "231465  231466  B0000BXJIS  A3U62RE5XZDP0G                   Marty   \n",
       "427827  427828  B008FHUFAU   AOXC0JQQZGGB6         Kenneth Shevlin   \n",
       "433954  433955  B006BXV14E  A3PWPNZVMNX3PA             rareoopdvds   \n",
       "70260    70261  B007I7Z3Z0  A1XNZ7PCE45KK7                  Og8ys1   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "165256                     0                       0      5  1268179200   \n",
       "231465                     0                       0      5  1298937600   \n",
       "427827                     0                       2      3  1224028800   \n",
       "433954                     0                       1      2  1335312000   \n",
       "70260                      0                       2      5  1334707200   \n",
       "\n",
       "                                             Summary  \\\n",
       "165256  Crunchy & Good Gluten-Free Sandwich Cookies!   \n",
       "231465                            great kitty treats   \n",
       "427827                                  COFFEE TASTE   \n",
       "433954              So the Mini-Wheats were too big?   \n",
       "70260                              Great Taste . . .   \n",
       "\n",
       "                                                     Text  HelpfulnessRatio  \\\n",
       "165256  Having tried a couple of other brands of glute...               0.0   \n",
       "231465  My cat loves these treats. If ever I can't fin...               0.0   \n",
       "427827  A little less than I expected.  It tends to ha...               0.0   \n",
       "433954  First there was Frosted Mini-Wheats, in origin...               0.0   \n",
       "70260   and I want to congratulate the graphic artist ...               0.0   \n",
       "\n",
       "       ReviewTime                                        CleanedText  \\\n",
       "165256 2010-03-10  having tried a couple of other brands of glute...   \n",
       "231465 2011-03-01  my cat loves these treats if ever i cant find ...   \n",
       "427827 2008-10-15  a little less than i expected  it tends to hav...   \n",
       "433954 2012-04-25  first there was frosted miniwheats in original...   \n",
       "70260  2012-04-18  and i want to congratulate the graphic artist ...   \n",
       "\n",
       "                                   CleanedSummary  \\\n",
       "165256  crunchy  good glutenfree sandwich cookies   \n",
       "231465                         great kitty treats   \n",
       "427827                               coffee taste   \n",
       "433954             so the miniwheats were too big   \n",
       "70260                              great taste      \n",
       "\n",
       "                                            TokenizedText  \\\n",
       "165256  [tried, couple, brands, glutenfree, sandwich, ...   \n",
       "231465  [cat, loves, treats, ever, cant, find, house, ...   \n",
       "427827  [little, less, expected, tends, muddy, taste, ...   \n",
       "433954  [first, frosted, miniwheats, original, size, f...   \n",
       "70260   [want, congratulate, graphic, artist, putting,...   \n",
       "\n",
       "                                      TokenizedSummary  \\\n",
       "165256  [crunchy, good, glutenfree, sandwich, cookies]   \n",
       "231465                          [great, kitty, treats]   \n",
       "427827                                 [coffee, taste]   \n",
       "433954                               [miniwheats, big]   \n",
       "70260                                   [great, taste]   \n",
       "\n",
       "                                           LemmatizedText  \\\n",
       "165256  [tried, couple, brand, glutenfree, sandwich, c...   \n",
       "231465  [cat, love, treat, ever, cant, find, house, po...   \n",
       "427827  [little, le, expected, tends, muddy, taste, ex...   \n",
       "433954  [first, frosted, miniwheats, original, size, f...   \n",
       "70260   [want, congratulate, graphic, artist, putting,...   \n",
       "\n",
       "                                   LemmatizedSummary  \\\n",
       "165256  [crunchy, good, glutenfree, sandwich, cooky]   \n",
       "231465                         [great, kitty, treat]   \n",
       "427827                               [coffee, taste]   \n",
       "433954                             [miniwheats, big]   \n",
       "70260                                 [great, taste]   \n",
       "\n",
       "                                    StemmedSummary  \\\n",
       "165256  crunchi & good gluten-fre sandwich cooki !   \n",
       "231465                           great kitti treat   \n",
       "427827                                  coffe tast   \n",
       "433954            so the mini-wheat were too big ?   \n",
       "70260                             great tast . . .   \n",
       "\n",
       "                                              StemmedText  \n",
       "165256  have tri a coupl of other brand of gluten-fre ...  \n",
       "231465  my cat love these treat . if ever i ca n't fin...  \n",
       "427827  a littl less than i expect . it tend to have a...  \n",
       "433954  first there wa frost mini-wheat , in origin si...  \n",
       "70260   and i want to congratul the graphic artist for...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d45c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "amazon_data.to_csv('amazon_data_wrangling.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd067b8",
   "metadata": {},
   "source": [
    "## 2.14 Summary<a id='2.14_Summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9291dde",
   "metadata": {},
   "source": [
    "The above steps provide a comprehensive guide for wrangling the  \"Amazon Fine Food Reviews\" dataset. The process includes handling missing values, creating new features, preprocessing text data, tokenizing and lemmatizing, encoding categorical variables, and saving the cleaned data. This ensures the dataset is in a usable format for further analysis and modeling tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
