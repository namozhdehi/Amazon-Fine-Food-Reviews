{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759e86df",
   "metadata": {},
   "source": [
    "# 4 Pre-Processing and Training Data<a id='4_Pre-Processing_and_Training_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5170295",
   "metadata": {},
   "source": [
    "## 4.1 Contents<a id='4.1_Contents'></a>\n",
    "* [4 Pre-Processing and Training Data](#4_Pre-Processing_and_Training_Data)\n",
    "  * [4.1 Contents](#4.1_Contents)\n",
    "  * [4.2 Introduction](#4.2_Introduction)\n",
    "  * [4.3 Imports](#4.3_Imports)\n",
    "  * [4.4 Load Data](#4.4_Load_Data)\n",
    "  * [4.5 Explore the Data](#4.5_Explore_Data)\n",
    "  * [4.6 Pre-processing](#4.6_Pre_processing)\n",
    "    * [4.6.1 Handle missing values](#4.6_1_Missing)   \n",
    "    * [4.6.2 Duplicate Records](#4.6_2_Duplicate)      \n",
    "    * [4.6.3 Feature Selection](#4.6_3_Feature)     \n",
    "    * [4.6.4 Text Vectorization](#4.6_4_Vectorization)      \n",
    "  * [4.7 Train-Test Split](#4.7_Split)    \n",
    "  * [4.8 Save dataframel](#4.8_Save)   \n",
    "  * [4.9 Summary](#4.9_Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35ea3f",
   "metadata": {},
   "source": [
    "## 4.2 Introduction<a id='4.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501e12e",
   "metadata": {},
   "source": [
    "This notebook focuses on the pre-processing and training steps for the Amazon Fine Food Reviews dataset. The dataset includes various features such as review text, scoring, helpfulness, and user profiles, with the goal of predicting whether a review is fake. The pre-processing steps involve handling missing values, creating new features, and vectorizing text data using TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38500f05",
   "metadata": {},
   "source": [
    "## 4.3 Imports<a id='4.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37678c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\armeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\armeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b099e8f",
   "metadata": {},
   "source": [
    "## 4.4 Load Data<a id='4.4_Load_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d9c573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>...</th>\n",
       "      <th>TokenizedText</th>\n",
       "      <th>TokenizedSummary</th>\n",
       "      <th>LemmatizedText</th>\n",
       "      <th>LemmatizedSummary</th>\n",
       "      <th>StemmedSummary</th>\n",
       "      <th>StemmedText</th>\n",
       "      <th>FakeReviews</th>\n",
       "      <th>TextLength</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165257</td>\n",
       "      <td>B000EVG8J2</td>\n",
       "      <td>A1L01D2BD3RKVO</td>\n",
       "      <td>B. Miller \"pet person\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1268179200</td>\n",
       "      <td>Crunchy &amp; Good Gluten-Free Sandwich Cookies!</td>\n",
       "      <td>Having tried a couple of other brands of glute...</td>\n",
       "      <td>...</td>\n",
       "      <td>['tried', 'couple', 'brands', 'glutenfree', 's...</td>\n",
       "      <td>['crunchy', 'good', 'glutenfree', 'sandwich', ...</td>\n",
       "      <td>['tried', 'couple', 'brand', 'glutenfree', 'sa...</td>\n",
       "      <td>['crunchy', 'good', 'glutenfree', 'sandwich', ...</td>\n",
       "      <td>crunchi &amp; good gluten-fre sandwich cooki !</td>\n",
       "      <td>have tri a coupl of other brand of gluten-fre ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>485</td>\n",
       "      <td>84</td>\n",
       "      <td>0.319318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231466</td>\n",
       "      <td>B0000BXJIS</td>\n",
       "      <td>A3U62RE5XZDP0G</td>\n",
       "      <td>Marty</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1298937600</td>\n",
       "      <td>great kitty treats</td>\n",
       "      <td>My cat loves these treats. If ever I can't fin...</td>\n",
       "      <td>...</td>\n",
       "      <td>['cat', 'loves', 'treats', 'ever', 'cant', 'fi...</td>\n",
       "      <td>['great', 'kitty', 'treats']</td>\n",
       "      <td>['cat', 'love', 'treat', 'ever', 'cant', 'find...</td>\n",
       "      <td>['great', 'kitty', 'treat']</td>\n",
       "      <td>great kitti treat</td>\n",
       "      <td>my cat love these treat . if ever i ca n't fin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490</td>\n",
       "      <td>99</td>\n",
       "      <td>0.435370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>427828</td>\n",
       "      <td>B008FHUFAU</td>\n",
       "      <td>AOXC0JQQZGGB6</td>\n",
       "      <td>Kenneth Shevlin</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1224028800</td>\n",
       "      <td>COFFEE TASTE</td>\n",
       "      <td>A little less than I expected.  It tends to ha...</td>\n",
       "      <td>...</td>\n",
       "      <td>['little', 'less', 'expected', 'tends', 'muddy...</td>\n",
       "      <td>['coffee', 'taste']</td>\n",
       "      <td>['little', 'le', 'expected', 'tends', 'muddy',...</td>\n",
       "      <td>['coffee', 'taste']</td>\n",
       "      <td>coffe tast</td>\n",
       "      <td>a littl less than i expect . it tend to have a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.010833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>433955</td>\n",
       "      <td>B006BXV14E</td>\n",
       "      <td>A3PWPNZVMNX3PA</td>\n",
       "      <td>rareoopdvds</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1335312000</td>\n",
       "      <td>So the Mini-Wheats were too big?</td>\n",
       "      <td>First there was Frosted Mini-Wheats, in origin...</td>\n",
       "      <td>...</td>\n",
       "      <td>['first', 'frosted', 'miniwheats', 'original',...</td>\n",
       "      <td>['miniwheats', 'big']</td>\n",
       "      <td>['first', 'frosted', 'miniwheats', 'original',...</td>\n",
       "      <td>['miniwheats', 'big']</td>\n",
       "      <td>so the mini-wheat were too big ?</td>\n",
       "      <td>first there wa frost mini-wheat , in origin si...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1631</td>\n",
       "      <td>294</td>\n",
       "      <td>0.159401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70261</td>\n",
       "      <td>B007I7Z3Z0</td>\n",
       "      <td>A1XNZ7PCE45KK7</td>\n",
       "      <td>Og8ys1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1334707200</td>\n",
       "      <td>Great Taste . . .</td>\n",
       "      <td>and I want to congratulate the graphic artist ...</td>\n",
       "      <td>...</td>\n",
       "      <td>['want', 'congratulate', 'graphic', 'artist', ...</td>\n",
       "      <td>['great', 'taste']</td>\n",
       "      <td>['want', 'congratulate', 'graphic', 'artist', ...</td>\n",
       "      <td>['great', 'taste']</td>\n",
       "      <td>great tast . . .</td>\n",
       "      <td>and i want to congratul the graphic artist for...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>649</td>\n",
       "      <td>122</td>\n",
       "      <td>0.235565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId             ProfileName  \\\n",
       "0  165257  B000EVG8J2  A1L01D2BD3RKVO  B. Miller \"pet person\"   \n",
       "1  231466  B0000BXJIS  A3U62RE5XZDP0G                   Marty   \n",
       "2  427828  B008FHUFAU   AOXC0JQQZGGB6         Kenneth Shevlin   \n",
       "3  433955  B006BXV14E  A3PWPNZVMNX3PA             rareoopdvds   \n",
       "4   70261  B007I7Z3Z0  A1XNZ7PCE45KK7                  Og8ys1   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      5  1268179200   \n",
       "1                     0                       0      5  1298937600   \n",
       "2                     0                       2      3  1224028800   \n",
       "3                     0                       1      2  1335312000   \n",
       "4                     0                       2      5  1334707200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0  Crunchy & Good Gluten-Free Sandwich Cookies!   \n",
       "1                            great kitty treats   \n",
       "2                                  COFFEE TASTE   \n",
       "3              So the Mini-Wheats were too big?   \n",
       "4                             Great Taste . . .   \n",
       "\n",
       "                                                Text  ...  \\\n",
       "0  Having tried a couple of other brands of glute...  ...   \n",
       "1  My cat loves these treats. If ever I can't fin...  ...   \n",
       "2  A little less than I expected.  It tends to ha...  ...   \n",
       "3  First there was Frosted Mini-Wheats, in origin...  ...   \n",
       "4  and I want to congratulate the graphic artist ...  ...   \n",
       "\n",
       "                                       TokenizedText  \\\n",
       "0  ['tried', 'couple', 'brands', 'glutenfree', 's...   \n",
       "1  ['cat', 'loves', 'treats', 'ever', 'cant', 'fi...   \n",
       "2  ['little', 'less', 'expected', 'tends', 'muddy...   \n",
       "3  ['first', 'frosted', 'miniwheats', 'original',...   \n",
       "4  ['want', 'congratulate', 'graphic', 'artist', ...   \n",
       "\n",
       "                                    TokenizedSummary  \\\n",
       "0  ['crunchy', 'good', 'glutenfree', 'sandwich', ...   \n",
       "1                       ['great', 'kitty', 'treats']   \n",
       "2                                ['coffee', 'taste']   \n",
       "3                              ['miniwheats', 'big']   \n",
       "4                                 ['great', 'taste']   \n",
       "\n",
       "                                      LemmatizedText  \\\n",
       "0  ['tried', 'couple', 'brand', 'glutenfree', 'sa...   \n",
       "1  ['cat', 'love', 'treat', 'ever', 'cant', 'find...   \n",
       "2  ['little', 'le', 'expected', 'tends', 'muddy',...   \n",
       "3  ['first', 'frosted', 'miniwheats', 'original',...   \n",
       "4  ['want', 'congratulate', 'graphic', 'artist', ...   \n",
       "\n",
       "                                   LemmatizedSummary  \\\n",
       "0  ['crunchy', 'good', 'glutenfree', 'sandwich', ...   \n",
       "1                        ['great', 'kitty', 'treat']   \n",
       "2                                ['coffee', 'taste']   \n",
       "3                              ['miniwheats', 'big']   \n",
       "4                                 ['great', 'taste']   \n",
       "\n",
       "                               StemmedSummary  \\\n",
       "0  crunchi & good gluten-fre sandwich cooki !   \n",
       "1                           great kitti treat   \n",
       "2                                  coffe tast   \n",
       "3            so the mini-wheat were too big ?   \n",
       "4                            great tast . . .   \n",
       "\n",
       "                                         StemmedText FakeReviews TextLength  \\\n",
       "0  have tri a coupl of other brand of gluten-fre ...         0.0        485   \n",
       "1  my cat love these treat . if ever i ca n't fin...         0.0        490   \n",
       "2  a littl less than i expect . it tend to have a...         0.0        136   \n",
       "3  first there wa frost mini-wheat , in origin si...         0.0       1631   \n",
       "4  and i want to congratul the graphic artist for...         0.0        649   \n",
       "\n",
       "  WordCount Sentiment  \n",
       "0        84  0.319318  \n",
       "1        99  0.435370  \n",
       "2        28 -0.010833  \n",
       "3       294  0.159401  \n",
       "4       122  0.235565  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data = pd.read_csv('amazon_data_eda.csv')\n",
    "amazon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78b552",
   "metadata": {},
   "source": [
    "## 4.5 Explore the Data<a id='4.5_Explore_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc3633a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>HelpfulnessRatio</th>\n",
       "      <th>FakeReviews</th>\n",
       "      <th>TextLength</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>284099.217770</td>\n",
       "      <td>1.771590</td>\n",
       "      <td>2.258040</td>\n",
       "      <td>4.189880</td>\n",
       "      <td>1.296134e+09</td>\n",
       "      <td>0.408624</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>436.306800</td>\n",
       "      <td>80.260440</td>\n",
       "      <td>0.243103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>163871.960801</td>\n",
       "      <td>7.846071</td>\n",
       "      <td>8.526657</td>\n",
       "      <td>1.306868</td>\n",
       "      <td>4.807836e+07</td>\n",
       "      <td>0.462236</td>\n",
       "      <td>0.030318</td>\n",
       "      <td>445.663953</td>\n",
       "      <td>79.598471</td>\n",
       "      <td>0.225739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.444384e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>142751.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.271203e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.107143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>283915.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.310947e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>425465.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.332634e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.374653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568446.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>593.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11321.000000</td>\n",
       "      <td>1901.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "count  100000.000000         100000.000000           100000.000000   \n",
       "mean   284099.217770              1.771590                2.258040   \n",
       "std    163871.960801              7.846071                8.526657   \n",
       "min         3.000000              0.000000                0.000000   \n",
       "25%    142751.000000              0.000000                0.000000   \n",
       "50%    283915.000000              0.000000                1.000000   \n",
       "75%    425465.750000              2.000000                2.000000   \n",
       "max    568446.000000            580.000000              593.000000   \n",
       "\n",
       "               Score          Time  HelpfulnessRatio    FakeReviews  \\\n",
       "count  100000.000000  1.000000e+05     100000.000000  100000.000000   \n",
       "mean        4.189880  1.296134e+09          0.408624       0.000920   \n",
       "std         1.306868  4.807836e+07          0.462236       0.030318   \n",
       "min         1.000000  9.444384e+08          0.000000       0.000000   \n",
       "25%         4.000000  1.271203e+09          0.000000       0.000000   \n",
       "50%         5.000000  1.310947e+09          0.000000       0.000000   \n",
       "75%         5.000000  1.332634e+09          1.000000       0.000000   \n",
       "max         5.000000  1.351210e+09          3.000000       1.000000   \n",
       "\n",
       "          TextLength      WordCount      Sentiment  \n",
       "count  100000.000000  100000.000000  100000.000000  \n",
       "mean      436.306800      80.260440       0.243103  \n",
       "std       445.663953      79.598471       0.225739  \n",
       "min        30.000000       4.000000      -1.000000  \n",
       "25%       179.000000      33.000000       0.107143  \n",
       "50%       301.000000      56.000000       0.233333  \n",
       "75%       528.000000      98.000000       0.374653  \n",
       "max     11321.000000    1901.000000       1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8009e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720989bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName                0\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                    9\n",
       "Text                       0\n",
       "HelpfulnessRatio           0\n",
       "ReviewTime                 0\n",
       "CleanedText                0\n",
       "CleanedSummary            46\n",
       "TokenizedText              0\n",
       "TokenizedSummary           0\n",
       "LemmatizedText             0\n",
       "LemmatizedSummary          0\n",
       "StemmedSummary             9\n",
       "StemmedText                0\n",
       "FakeReviews                0\n",
       "TextLength                 0\n",
       "WordCount                  0\n",
       "Sentiment                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc87815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   Id                      100000 non-null  int64  \n",
      " 1   ProductId               100000 non-null  object \n",
      " 2   UserId                  100000 non-null  object \n",
      " 3   ProfileName             100000 non-null  object \n",
      " 4   HelpfulnessNumerator    100000 non-null  int64  \n",
      " 5   HelpfulnessDenominator  100000 non-null  int64  \n",
      " 6   Score                   100000 non-null  int64  \n",
      " 7   Time                    100000 non-null  int64  \n",
      " 8   Summary                 99991 non-null   object \n",
      " 9   Text                    100000 non-null  object \n",
      " 10  HelpfulnessRatio        100000 non-null  float64\n",
      " 11  ReviewTime              100000 non-null  object \n",
      " 12  CleanedText             100000 non-null  object \n",
      " 13  CleanedSummary          99954 non-null   object \n",
      " 14  TokenizedText           100000 non-null  object \n",
      " 15  TokenizedSummary        100000 non-null  object \n",
      " 16  LemmatizedText          100000 non-null  object \n",
      " 17  LemmatizedSummary       100000 non-null  object \n",
      " 18  StemmedSummary          99991 non-null   object \n",
      " 19  StemmedText             100000 non-null  object \n",
      " 20  FakeReviews             100000 non-null  float64\n",
      " 21  TextLength              100000 non-null  int64  \n",
      " 22  WordCount               100000 non-null  int64  \n",
      " 23  Sentiment               100000 non-null  float64\n",
      "dtypes: float64(3), int64(7), object(14)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "amazon_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5927ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id has value counts -- 100000\n",
      "ProductId has value counts -- 31698\n",
      "UserId has value counts -- 71536\n",
      "ProfileName has value counts -- 64663\n",
      "HelpfulnessNumerator has value counts -- 148\n",
      "HelpfulnessDenominator has value counts -- 156\n",
      "Score has value counts -- 5\n",
      "Time has value counts -- 2699\n",
      "Summary has value counts -- 71666\n",
      "Text has value counts -- 87592\n",
      "HelpfulnessRatio has value counts -- 478\n",
      "ReviewTime has value counts -- 2699\n",
      "CleanedText has value counts -- 87584\n",
      "CleanedSummary has value counts -- 63533\n",
      "TokenizedText has value counts -- 87554\n",
      "TokenizedSummary has value counts -- 58653\n",
      "LemmatizedText has value counts -- 87554\n",
      "LemmatizedSummary has value counts -- 58046\n",
      "StemmedSummary has value counts -- 66799\n",
      "StemmedText has value counts -- 87565\n",
      "FakeReviews has value counts -- 2\n",
      "TextLength has value counts -- 2689\n",
      "WordCount has value counts -- 721\n",
      "Sentiment has value counts -- 32836\n"
     ]
    }
   ],
   "source": [
    "for x in amazon_data.columns:\n",
    "    print(f'{x} has value counts -- {amazon_data[x].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81f658",
   "metadata": {},
   "source": [
    "## 4.6 Pre-processing<a id='4.6_Pre_processing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cfc4d8",
   "metadata": {},
   "source": [
    "### 4.6.1 Handle missing values<a id='4.6_1_Missing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef558577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName                0\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                    9\n",
      "Text                       0\n",
      "HelpfulnessRatio           0\n",
      "ReviewTime                 0\n",
      "CleanedText                0\n",
      "CleanedSummary            46\n",
      "TokenizedText              0\n",
      "TokenizedSummary           0\n",
      "LemmatizedText             0\n",
      "LemmatizedSummary          0\n",
      "StemmedSummary             9\n",
      "StemmedText                0\n",
      "FakeReviews                0\n",
      "TextLength                 0\n",
      "WordCount                  0\n",
      "Sentiment                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = amazon_data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c25d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'HelpfulnessRatio' column with 0\n",
    "amazon_data['HelpfulnessRatio'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "481f2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in critical columns\n",
    "amazon_data = amazon_data.dropna(subset=['Summary', 'Text', 'CleanedSummary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebefa461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                        0\n",
      "ProductId                 0\n",
      "UserId                    0\n",
      "ProfileName               0\n",
      "HelpfulnessNumerator      0\n",
      "HelpfulnessDenominator    0\n",
      "Score                     0\n",
      "Time                      0\n",
      "Summary                   0\n",
      "Text                      0\n",
      "HelpfulnessRatio          0\n",
      "ReviewTime                0\n",
      "CleanedText               0\n",
      "CleanedSummary            0\n",
      "TokenizedText             0\n",
      "TokenizedSummary          0\n",
      "LemmatizedText            0\n",
      "LemmatizedSummary         0\n",
      "StemmedSummary            0\n",
      "StemmedText               0\n",
      "FakeReviews               0\n",
      "TextLength                0\n",
      "WordCount                 0\n",
      "Sentiment                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = amazon_data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f603636",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ca1e6a1",
   "metadata": {},
   "source": [
    "### 4.6.2 Handle Duplicate Records<a id='4.6_2_Duplicate'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94de3a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99954, 24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356de8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for duplicates, excluding 'Id'\n",
    "columns_no_id = [col for col in amazon_data.columns if col != 'Id']\n",
    "\n",
    "# Check for duplicate records based on all columns except 'Id'\n",
    "duplicate_records = amazon_data[amazon_data.duplicated(subset=columns_no_id, keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b164ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99928, 24)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data = amazon_data[~amazon_data.index.isin(duplicate_records.index)]\n",
    "amazon_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92168a72",
   "metadata": {},
   "source": [
    "### 4.6.3 Feature Selection<a id='4.6_3_Feature'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02eef1b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\armeh\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\armeh\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\armeh\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\armeh\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\armeh\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2021.11.10)\n",
      "Requirement already satisfied: joblib in c:\\users\\armeh\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\armeh\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob      \n",
    "\n",
    "import textblob           \n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaeac9c",
   "metadata": {},
   "source": [
    "## 4.7 Clean data<a id='4.6_3_Clean'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a61a621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id   ProductId          UserId             ProfileName  \\\n",
      "0  165257  B000EVG8J2  A1L01D2BD3RKVO  B. Miller \"pet person\"   \n",
      "1  231466  B0000BXJIS  A3U62RE5XZDP0G                   Marty   \n",
      "2  427828  B008FHUFAU   AOXC0JQQZGGB6         Kenneth Shevlin   \n",
      "3  433955  B006BXV14E  A3PWPNZVMNX3PA             rareoopdvds   \n",
      "4   70261  B007I7Z3Z0  A1XNZ7PCE45KK7                  Og8ys1   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     0                       0      5  1268179200   \n",
      "1                     0                       0      5  1298937600   \n",
      "2                     0                       2      3  1224028800   \n",
      "3                     0                       1      2  1335312000   \n",
      "4                     0                       2      5  1334707200   \n",
      "\n",
      "                                        Summary  \\\n",
      "0  Crunchy & Good Gluten-Free Sandwich Cookies!   \n",
      "1                            great kitty treats   \n",
      "2                                  COFFEE TASTE   \n",
      "3              So the Mini-Wheats were too big?   \n",
      "4                             Great Taste . . .   \n",
      "\n",
      "                                                Text  ...  \\\n",
      "0  Having tried a couple of other brands of glute...  ...   \n",
      "1  My cat loves these treats. If ever I can't fin...  ...   \n",
      "2  A little less than I expected.  It tends to ha...  ...   \n",
      "3  First there was Frosted Mini-Wheats, in origin...  ...   \n",
      "4  and I want to congratulate the graphic artist ...  ...   \n",
      "\n",
      "                                       TokenizedText  \\\n",
      "0  [tried, couple, brands, glutenfree, sandwich, ...   \n",
      "1  [cat, loves, treats, ever, cant, find, house, ...   \n",
      "2  [little, less, expected, tends, muddy, taste, ...   \n",
      "3  [first, frosted, miniwheats, original, size, f...   \n",
      "4  [want, congratulate, graphic, artist, putting,...   \n",
      "\n",
      "                                 TokenizedSummary  \\\n",
      "0  [crunchy, good, glutenfree, sandwich, cookies]   \n",
      "1                          [great, kitty, treats]   \n",
      "2                                 [coffee, taste]   \n",
      "3                               [miniwheats, big]   \n",
      "4                                  [great, taste]   \n",
      "\n",
      "                                      LemmatizedText  \\\n",
      "0  [tried, couple, brand, glutenfree, sandwich, c...   \n",
      "1  [cat, love, treat, ever, cant, find, house, po...   \n",
      "2  [little, le, expected, tends, muddy, taste, ex...   \n",
      "3  [first, frosted, miniwheats, original, size, f...   \n",
      "4  [want, congratulate, graphic, artist, putting,...   \n",
      "\n",
      "                              LemmatizedSummary  \\\n",
      "0  [crunchy, good, glutenfree, sandwich, cooky]   \n",
      "1                         [great, kitty, treat]   \n",
      "2                               [coffee, taste]   \n",
      "3                             [miniwheats, big]   \n",
      "4                                [great, taste]   \n",
      "\n",
      "                                StemmedSummary  \\\n",
      "0  [crunchi, good, glutenfre, sandwich, cooki]   \n",
      "1                        [great, kitti, treat]   \n",
      "2                                [coffe, tast]   \n",
      "3                             [miniwheat, big]   \n",
      "4                                [great, tast]   \n",
      "\n",
      "                                         StemmedText FakeReviews TextLength  \\\n",
      "0  [tri, coupl, brand, glutenfre, sandwich, cooki...         0.0        485   \n",
      "1  [cat, love, treat, ever, cant, find, hous, pop...         0.0        490   \n",
      "2  [littl, less, expect, tend, muddi, tast, expec...         0.0        136   \n",
      "3  [first, frost, miniwheat, origin, size, frost,...         0.0       1631   \n",
      "4  [want, congratul, graphic, artist, put, entir,...         0.0        649   \n",
      "\n",
      "  WordCount Sentiment  \n",
      "0        84  0.319318  \n",
      "1        99  0.435370  \n",
      "2        28 -0.010833  \n",
      "3       294  0.159401  \n",
      "4       122  0.235565  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "amazon_data['TokenizedText'] = amazon_data['CleanedText'].apply(lambda x: x.split())\n",
    "amazon_data['TokenizedSummary'] = amazon_data['CleanedSummary'].apply(lambda x: x.split())\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "amazon_data['TokenizedText'] = amazon_data['TokenizedText'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "amazon_data['TokenizedSummary'] = amazon_data['TokenizedSummary'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Ensure text is not empty after removing stopwords\n",
    "amazon_data = amazon_data[amazon_data['TokenizedText'].str.len() > 0]\n",
    "\n",
    "# Lemmatize text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "amazon_data['LemmatizedText'] = amazon_data['TokenizedText'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "amazon_data['LemmatizedSummary'] = amazon_data['TokenizedSummary'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Stem text\n",
    "stemmer = PorterStemmer()\n",
    "amazon_data['StemmedText'] = amazon_data['TokenizedText'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "amazon_data['StemmedSummary'] = amazon_data['TokenizedSummary'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Calculate text length and word count\n",
    "amazon_data['TextLength'] = amazon_data['Text'].apply(len)\n",
    "amazon_data['WordCount'] = amazon_data['Text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Display the preprocessed dataframe\n",
    "print(amazon_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8645f",
   "metadata": {},
   "source": [
    "## 4.8 Train-Test Split<a id='4.8_Split'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e4516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X = amazon_data['LemmatizedText'].apply(lambda x: ' '.join(x))\n",
    "y = amazon_data['FakeReviews']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c87c3",
   "metadata": {},
   "source": [
    "### 4.9 Text Vectorization<a id='4.9_Vectorization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "834ea2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2915d5",
   "metadata": {},
   "source": [
    "## 4.8 Save dataframe<a id='4.8_Save'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcaa5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data and vectorizer\n",
    "amazon_data.to_csv('preprocessed_amazon_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49afd1",
   "metadata": {},
   "source": [
    "## 4.9 Summary<a id='4.9_Summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01491ca",
   "metadata": {},
   "source": [
    "This notebook covers the preprocessing steps for the Amazon Fine Food Reviews dataset to detect fake reviews. We began by loading the data and performing exploratory data analysis to identify missing values and duplicates.\n",
    "\n",
    "Key preprocessing steps included handling missing values, cleaning and tokenizing text, removing stopwords, lemmatizing, and stemming. We also calculated additional features like text length and word count.\n",
    "\n",
    "The data was then split into training and testing sets, and text vectorization was performed using TF-IDF. Finally, the preprocessed data was saved for further analysis and model training.\n",
    "\n",
    "These steps ensure the data is clean and ready for effective machine learning model training to detect fake reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
